# __CS224n: Natural Language Processing with Deep Learning__

## Course links

- Course page [CS224N](http://web.stanford.edu/class/cs224n/).
- Lecture videos 2020: (Updating).

## Table of contents

- __Week 1__: 
    - [ ] Introduction and Word Vectors.
    - [ ] Gensim word vectors example.
    - [ ] Word Vectors 2 and Word Senses.
    - [ ] Python review session.
- __Week 2__:
    - [ ] Word Window Classification, Neural Networks, and Pytorch.
    - [ ] Matrix Calculus and Backpropagation.
- __Week 3__:
    - [ ] Linguistic Structure: Dependency Parsing.
    - [ ] The probability of a sentence? Recurrent Neural Networks and Language Models 
- __Week 4__:
    - [ ] Vanishing Gradients and Fancy RNNs.
    - [ ] Machine Translation, Seq2seq and Attention.
- __Week 5__:
    - [ ] Practical Tips for Final Projects.
    - [ ] Question Answering and the Default Final Project.
- __Week 6__:
    - [ ] ConvNets for NLP.
    - [ ] Information from parts of words (Subword Models) and Transformer architectures.
- __Week 7__:
    - [ ] Contextual Word Representation: BERT.
    - [ ] Modeling contexts of use: Contextual Representations and Pretraining.
- __Week 8__:
    - [ ] Natural Language Generation.
    - [ ] Reference in Language and Coreference Resolution.
- __Week 9__:
    - [ ] Fairness and Inclusion in AI.
    - [ ] Constituency Parsing and Tree Recursive Neural Networks.
- __Week 10__:
    - [ ] Recent Advances in Low Resource Machine Translation.
    - [ ] Future of NLP + Deep Learning.
- __Week 11__:
    - [ ] Final project poster session.

## Contributors:

- [Viet-Tien](https://github.com/tiena2cva)
- [honghanhh](https://github.com/honghanhh)

