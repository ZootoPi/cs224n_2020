# Machine Translation, Seq2seq and Attention

- [Slide](http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture08-nmt.pdf)
- [Note](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf)
- Suggested Readings:
    1. [Statistical Machine Translation slides, CS224n 2015](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/syllabus.shtml) (lectures 2/3/4)
    2. [Statistical Machine Translation](https://www.cambridge.org/core/books/statistical-machine-translation/94EADF9F680558E13BE759997553CDE5)
    3. [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf)
    4. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
    5. [Sequence Transduction with Recurrent Neural Networks](https://arxiv.org/pdf/1211.3711.pdf)
    6. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
    7. [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)
    8. [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf)
